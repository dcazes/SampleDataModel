import tensorflow as tf
from keras.models import Sequential
from keras.layers import LSTM, LeakyReLU, MaxPooling1D, GRU
from keras import optimizers
from sklearn.model_selection import train_test_split
import numpy as np
import matplotlib.pyplot as plt
import matplotlib.font_manager as fm
from scipy import signal

font_prop = fm.FontProperties(size=16)
-----------------------------------------------
#Scan Rate in Seconds per Sample
SR = (7.5 * 60) / 100

#Samples per Patient
numSam = 200

#Heart Rate in BPM
HR = 73

#Number of Patients
numPat = 250

#Breathing Rate

#Breath Hold On (sec)
bhOn = 15

#Breath Hold Off (sec)
bhOff = 45
-----------------------------------------------
#Random Amplitude Generation
A = [np.random.random() for i in range (numPat)]
B = [np.random.random() for i in range (numPat)]
C = [np.random.random() for i in range (testSize)]
D = [np.random.random() for i in range (testSize)]

#Time Series Creation
x = [[[(A[j] * np.sin((2 * np.pi * (HR / 60)) * i)) + 
       (B[j] * np.cos((2 * np.pi * (HR / 60)) * i )) + 
       (signal.square (((2/(bhOn + bhOff)) * np.pi * i), duty = (bhOn/(bhOn + bhOff))))] for i in np.arange (0, SR * numSam, SR)] for j in range (numPat)]
         
y = [(A[i] + B[i])/2 for i in range (numPat)]

#Unseen Data
x_test = [[[(C[j] * np.sin((2 * np.pi * (HR / 60)) * i)) + 
            (D[j] * np.cos((2 * np.pi * (HR / 60)) * i)) + 
            (signal.square (((2/(bhOn + bhOff)) * np.pi * i), duty = (bhOn/(bhOn + bhOff))))] for i in np.arange (0, SR * numSam, SR)] for j in range (50)]

y_test = [(C[i] + D[i])/2 for i in range (testSize)]
------------------------------------------------
#Convert Lists to Arrays
X = np.array(x, dtype=float)
Y = np.array(y, dtype=float)
X_test = np.array(x_test, dtype=float)
Y_test = np.array(y_test, dtype=float)
#Linear Scaling Between 0-1 (Max-Min Normalization)
samMin = np.amin (X, axis = 1)
samMax = np.amax (X, axis = 1)

testMin = np.amin (X_test, axis = 1)
testMax = np.amax (X_test, axis = 1)

for i in range (numPat):
    for j in range (numSam):
        X[i, j] = X[i, j] - samMin[i]
        X[i, j] = X[i, j] / (samMax[i] - samMin[i])

for i in range (testSize):
    for j in range (numSam):
        X_test[i, j] = X_test[i, j] - testMin[i]
        X_test[i, j] = X_test[i, j] / (testMax[i] - testMin[i])
--------------------------------------------------------------
#Plotting Normalized Data
plt.figure(figsize=(15,3))
plt.plot (X[31, :, 0])
plt.plot (X[30, :, 0])
plt.xlabel ("Sample", fontproperties = font_prop)
plt.ylabel("Output (Normalized)", fontproperties = font_prop)
-----------------------------------------------------------------
#Adding Noise
noise = np.random.normal(loc = 0, scale = 0.05, size = (numPat, numSam))
for i in range (numPat):
    for j in range (numSam):
         X[i, j] = X[i, j] + noise[i, j]
            
noise = np.random.normal(loc = 0, scale = 0.05, size = (testSize, numSam))
for i in range (testSize):
    for j in range (numSam):
         X_test[i, j] = X[i, j] + noise[i, j]
------------------------------------------------------------------------
#Linear Scaling Between 0-1 (Max-Min Normalization)
samMin = np.amin (X, axis = 1)
samMax = np.amax (X, axis = 1)

testMin = np.amin (X_test, axis = 1)
testMax = np.amax (X_test, axis = 1)

for i in range (numPat):
    for j in range (numSam):
        X[i, j] = X[i, j] - samMin[i]
        X[i, j] = X[i, j] / (samMax[i] - samMin[i])

for i in range (testSize):
    for j in range (numSam):
        X_test[i, j] = X_test[i, j] - testMin[i]
        X_test[i, j] = X_test[i, j] / (testMax[i] - testMin[i])
---------------------------------------------------------------
#Plotting Normalized Data
plt.figure(figsize=(15,3))
plt.plot (X[31, :, 0])
plt.plot (X[30, :, 0])
plt.xlabel ("Sample", fontproperties = font_prop)
plt.ylabel("Output (Normalized)", fontproperties = font_prop)
----------------------------------------------------------------
#Model
model = Sequential ()
opt = optimizers.Adam(lr=0.001, beta_1=0.9, beta_2=0.999, epsilon=None, decay=0.0, amsgrad=False)

model.add(GRU((10), return_sequences=True, batch_input_shape = (None, numSam, 1)))
model.add(LeakyReLU(alpha = 0.2))
model.add(GRU((10), return_sequences=True))
model.add(LeakyReLU(alpha = 0.2))
model.add(GRU((10), return_sequences=True))
model.add(LeakyReLU(alpha = 0.2))
model.add(GRU((1), return_sequences=False))
model.add(LeakyReLU(alpha = 0.2))
model.compile(loss = 'mean_absolute_error', optimizer = opt)
model.summary()
-----------------------------------------------------------------------
#Validation Data Splitting
x_train, x_val, y_train, y_val = train_test_split(X, Y, test_size = 0.2, random_state = 1)

#Tracking Average Loss Through Each Epoch
history = model.fit(x_train, y_train, epochs= 120,  validation_data=(x_val, y_val))

#Testing the Model on Unseen Dataset
predictions = model.predict (X_test)
------------------------------------------------------------
#Plotting Predictions vs Actual Values
plt.figure(figsize=(6,5))
plt.scatter (range(testSize), predictions, c='r')
plt.scatter (range(testSize), Y_test, c='g')
plt.xlabel ("Test Set", fontproperties = font_prop)
plt.ylabel ("Output", fontproperties = font_prop)
plt.legend (['Prediction', 'Actual'], loc = 4)
plt.show()

#Plotting Loss Over Epochs
plt.figure(figsize=(6,5))
plt.plot(history.history['loss'])
plt.plot(history.history['val_loss'])
plt.xlabel("Epoch", fontproperties = font_prop)
plt.ylabel("Loss", fontproperties = font_prop)
plt.legend (['Training', 'Validation'], loc = 1)
plt.show()
------------------------------------------------------
error = [[Y_test[i] - predictions [i]] for i in range (testSize)]
error = np.array(error, dtype=float)
print (np.mean(error))
